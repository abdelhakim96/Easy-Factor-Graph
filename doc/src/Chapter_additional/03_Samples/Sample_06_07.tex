
\section{Sample 06: Learning, part A}

The aim of this series of examples is to show how to perform the learning of factor graphs.
In all the examples contained in this Section, learning is done with the following methodology.
A Gibbs sampler is used to take samples from the joint distribution correlating all the variables in a model A. Model A is actually the model to learn. The training set obtained from model A, is used to train a model B. Model B has the same variables and factors of model A, but with different values for the free parameters $w_{1,2,\cdots}$ (Section \ref{sec:00:LEARN}). In this way, after having performed the learning, the value of the free parameters in model B will assume similar values to the ones in model A, showing the effectiveness of the functionalities contained in EFG. Clearly, this is not the approach followed in real applications, were the real coefficient of the model are unknown and only a training set of examples are available.

\subsection{part 01}
\label{sample_tuning}

\begin{figure}
	\centering
\def\svgwidth{0.3 \textwidth}
\import{../Chapter_additional/03_Samples/image_06_07/}{graph_2.pdf_tex} 
\caption{The graph considered by part 01.}
\label{fig:sample_06:0}
\end{figure}

Part 01 considers the loopy graph reported in Figure \ref{fig:sample_06:0}. $A$, $B$ and $C$ are all binary variables, while $\Psi_{AB}$, $\Psi_{AC}$ and $\Psi_{BC}$ are simple correlating exponential distributions having as weights, respectively, $\alpha$, $\beta$ and $\gamma$. 
\\
Some samples are generated from the model with a Gibbs sampling, in order to use it later as a training set.
All the weigths in the model are set to 1, and tuned using a quasi newton trainer that consider the previously generated training set.
The aim is to show that after training the values of the weights are similar to the initial ones.

\subsection{part 02}

\begin{figure}
	\centering
\def\svgwidth{0.5 \textwidth}
\import{../Chapter_additional/03_Samples/image_06_07/}{graph_3.pdf_tex} 
\caption{The graph considered by part 02.}
\label{fig:sample_06:1}
\end{figure}

Part 02 considers a structure made of both tunable and non-tunable factors. The considered structure is reported in Figure \ref{fig:sample_06:1}.
Weights $\beta$ and $\gamma$ must be tuned through learning, as similarly described in Section \ref{sample_tuning}, while $\alpha$ and $\gamma$ are constant and known (refer also to the formalism described in Figure \ref{fig:00:example}).

\subsection{part 03}

\begin{figure}
	\centering
\def\svgwidth{0.75 \textwidth}
\import{../Chapter_additional/03_Samples/image_06_07/}{graph_4.pdf_tex} 
\caption{The graph considered by part 03.}
\label{fig:sample_06:2}
\end{figure}

Part 03 considers the loopy structure reported in Section \ref{sec:Sample_03_part_04}. However, here instead of having constant exponential shapes, all the factors are made of tunable exponentials. The value assumed by the weight of model A (see the introduction of this Section) are showed in Figure \ref{fig:sample_06:2}.
Weights are tuned through learning as similarly described in Section \ref{sample_tuning}.

\subsection{part 04}

\begin{figure}
	\centering
\def\svgwidth{0.45 \textwidth}
\import{../Chapter_additional/03_Samples/image_06_07/}{graph_5.pdf_tex} 
\caption{The graph considered by part 04.}
\label{fig:sample_06:3}
\end{figure}

Part 04 considers the structure reported in Figure \ref{fig:sample_06:3}, for which all the potentials connecting pairs $Y_{i-1}, Y_i$ share the same weight $\alpha$, while the factors connecting pairs $X_i,Y_i$ share the weight $\beta$. The approach described
Weights are tuned through learning as similarly described in Section \ref{sample_tuning}.

\section{Sample 07: Learning, part B}

\begin{figure}
	\centering
\def\svgwidth{0.6 \textwidth}
\import{../Chapter_additional/03_Samples/image_06_07/}{graph_6.pdf_tex} 
\caption{The conditional random field considered in Sample 07.}
\label{fig:sample_07:0}
\end{figure}

The aim of this example is to show how the learning process can be done when dealing with Conditional random fields. In particular, the structure  reported in Figure \ref{fig:sample_07:0} is considered (values of the free parameters are not indicated, since the reader may refer to the sources provided).
\\
The approach adopted is similar to the one followed in the previous series of example, considering a couple of model A and B (see the initial part of the previous Section). However, in this case we cannot simply draw samples from the joint distribution correlating the variables in the model, since such a distribution does not exists. Indeed, the conditional random field of Figure \ref{fig:sample_07:0}, models the conditional distribution of variables $Y_{1,2,\cdots}$ w.r.t the evidences $X_{1,2,\cdots}$. For this reason, all the possible combination of evidences are determined, considering all $x \in \lbrace Dom(X_1) \cup Dom(X_2) \cup \cdots \rbrace$. For each $x$, samples from the conditioned distribution $\mathbb{P}(Y_{1,2,\cdots} | x)$ are taken with a Gibbs sampler. The entire population of samples determined is actually the training set adopted fro training the conditional random field in Figure \ref{fig:sample_07:0}.
 
